\section{Methods}
\subsection{Preprocessing}
\textit{Handling missing values}

There are three types of missing data in the dataset: NA, NULL and PrivacySuppressed. These values account for measurable percentage of the dataset, so it is not appropriate to simply remove them. A proportion of NULL values are meaningful as they indicate the absence of binary variables, or numerical zero. In order to use these data, we replace NULL values (except those in CDR3 column) by zero. Meanwhile we replace PrivacySuppressed value by NA, and later we will impute these values.

We remove the variables that contains over 30 percent of NAs, and then remove entries that contains 30 percent of NAs. That leaves about ( ) observations. Then we want to impute the rest of NA values through k nearest neighbors (KNN) with R package 'VIM'. Therefore, we prepare a full dataset for modeling. 

\textit{Select the predicting variable}

One objective of our model is to identify the key indicator of borrowing risk. In repayment section, we observe that there are several important predictors: one- and three-year Cohort Default Rate (CDR), and 1,3,5,7-year Repayment Rate (RPY_YR_RT). There are also data on repayment rate split on different categories e.g. degree completor v.s non-completor, income low v.s medium v.s high, etc. 

We first look at Repayment Rate in different categories of students. Below is the boxplot of one-year repayment rate with regard to categories. It shows the facts that whether students complete degree, students' family income level and whether students receive pell grants will affect the repayment rate significantly. The result is also shown by running two sample t-test on these categories.

<<results=tex>>==
library(png)
library(grid)
img <- readPNG("../../images/one-year-repayment-by-categories.png")
grid.raster(img)
@

There is also high correlations between default rate and repayment rate. The following is the correlation matrix among 1,3 year CDR and 1,3,5,7 year RPY_RT

<<results=tex>>==
cat(readLines('../../data/CDR_repayment_correlation.txt'), sep = '\n')
@

And the correlation plot: 

<<results=tex>>==
img <- readPNG("../../images/CDR_repayment_correlation.png")
grid.raster(img)
@

Therefore, we choose 3-year CDR as the indicator variable as our model. Firstly, it captures the debt conditon of students in a certain college because students who default on loan is either buried in debt or unable to make enough earnings after graduate. Secondly, it is highly correlated with repayment variables, which measure borrowers ability's pay back the loan. We only need to focus on one of them. We believe CDR3 is an important risk factor that credit instituion needs to consider before they borrow. 

\textit{Select predictors}

After removing unrelavant features, we narrow our focus down to the Earning section and Financial Aid Section. We observe the correlation plot. For earning section:

<<results=tex>>==
img <- readPNG("../../images/earning_correlation.png")
grid.raster(img)
@

For financial aid section:

<<results=tex>>==
img <- readPNG("../../images/financial_correlation.png")
grid.raster(img)
@

Based on the correlation plot, we retain variables with low correlation and drop those with high correlation. Finally we get a dataset that is of size (), with predictors being: .. and predicting variable being CDR3.

\subsection{Logistic Regression}
\textit{Logistic regression} is one of the most commonly used tools for applied statistics
and discrete data analysis. In this model, we have a binary output variable Y, and we want to model the conditional probability  
\begin{equation}
Pr\left(Y=1\,\middle|\,X = x\right)
\end{equation}
as a function of $ \textit{x} $; any unknown parameters in the function are to be estimated by maximum likelihood.\par
Formally, the model logistic regression model is that
\begin{equation}
log \frac{p(x)}{1-p(x)} = \beta_0 + x \cdot \beta
\end{equation}

Solving for p(x), this gives 
\begin{equation}
p(x) = \frac{e^{\beta_0+x\cdot\beta}}{1 + e^{\beta_0+x\cdot\beta}}
\end{equation}

To minimize the mis-classification rate, we should predict Y = 1 when p > 0.5
and Y = 0 when p < 0.5. This means guessing 1 whenever $ \beta_0+x\cdot\beta $ is non-negative,
and 0 otherwise. Therefore, the decision boundary separating the two predicted classes is the solution of $ \beta_0+x\cdot\beta = 0 $.


\subsection{Gradient Boosting Machines}
We also approached the problem from a regression point of view. The idea behind \textit{gradient boosting} is to combine weak learners in an iterative fashion in order to create a stronger model. We used decision trees as our base model, as they are the most popular for this method of learning. Our goal is to find a model \textbf{M} that predicts the label (in this case, our default rate)










